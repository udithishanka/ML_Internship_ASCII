{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autotrain-advanced 0.6.51 requires datasets[vision]~=2.14.0, but you have datasets 2.15.0 which is incompatible.\n",
      "autotrain-advanced 0.6.51 requires evaluate==0.3.0, but you have evaluate 0.4.1 which is incompatible.\n",
      "autotrain-advanced 0.6.51 requires fastapi==0.104.1, but you have fastapi 0.105.0 which is incompatible.\n",
      "autotrain-advanced 0.6.51 requires packaging==23.1, but you have packaging 23.2 which is incompatible.\n",
      "autotrain-advanced 0.6.51 requires protobuf==4.23.4, but you have protobuf 4.25.1 which is incompatible.\n",
      "autotrain-advanced 0.6.51 requires pydantic==2.4.2, but you have pydantic 1.10.13 which is incompatible.\n",
      "autotrain-advanced 0.6.51 requires tqdm==4.65.0, but you have tqdm 4.66.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install autoawq -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awq import AutoAWQCausalLM\n",
    "from transformers import Autotokenizer\n",
    "import torch\n",
    "\n",
    "model_path = 'PY007/TinyLlama-1.1B-Chat-v0.3'\n",
    "\n",
    "quant_name = model_path.split('/')[-1] + \"-AWQ\"\n",
    "\n",
    "quant_path = 'Trelis/' + quant_name\n",
    "quant_config = {\"zero_point\": True, \"q_group_size\": 128, \"w_bit\":4}\n",
    "\n",
    "#load model\n",
    "model = AutoAWQCausalLM.from_pretrained(model_path,device_map = 'auto')\n",
    "tokenizer = Autotokenizer.from_pretrained(model_path,trust_remote_code = True)\n",
    "\n",
    "#quantize\n",
    "model.quantize(tokenizer, quant_config = quant_config)\n",
    "\n",
    "#save quantized model\n",
    "model.save_quantized(quant_name, safetensors = True, shard_size = '10GB')\n",
    "tokenizer.save_pretrained(quant_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "#initialize the HfApi Class\n",
    "api = HfApi()\n",
    "\n",
    "# Specify the path where you want the file to be uploaded in the repository\n",
    "path_in_repo = 'model.safetensors'\n",
    "\n",
    "local_file_path = './'+ quant_name + \"/\" + path_in_repo\n",
    "\n",
    "#generate repo_id from model path\n",
    "repo_id = 'Trelis/' +quant_name\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj = local_file_path,\n",
    "    path_in_repo = path_in_repo,\n",
    "    repo_id = repo_id,\n",
    "    repo_type = \"model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload non model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "repo_id = 'Trelis/' + quant_name\n",
    "\n",
    "local_file_paths = [\n",
    "    \"./\" + quant_name + \"/config.jason\",\n",
    "    \"./\" + quant_name + \"/genreation_config.jason\",\n",
    "    \"./\" + quant_name + \"/quant_config.jason\",\n",
    "    \"./\" + quant_name + \"/special_tokens_map.jason\",\n",
    "    \"./\" + quant_name + \"/tokenizer_config.jason\",\n",
    "    \"./\" + quant_name + \"/tokenizer.jason\",\n",
    "]\n",
    "\n",
    "\n",
    "for local_file_path in local_file_paths:\n",
    "    file_name = local_file_path.split(\"/\")[-1]\n",
    "    path_in_repo = file_name\n",
    "\n",
    "    api.upload_file(\n",
    "        path_or_fileobj = local_file_path,\n",
    "        path_in_repo= path_in_repo,\n",
    "        repo_id= repo_id,\n",
    "        repo_type= \"model\",\n",
    "    )\n",
    "\n",
    "    print(f\"Uploaded {file_name} to {repo_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "model_name_or_path = \"Trelis/Llama-2-13b-chat-longlora-32-sft-AWQ\"\n",
    "\n",
    "model = AutoAWQCausalLM.from_quatized(model_name_or_path, fuse_layers = True,\n",
    "                                      trust_remote_code = False, safetensors = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What planets are in our solar system?\"\n",
    "\n",
    "formatted_prompt = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "tokens = tokenizer(\n",
    "    formatted_prompt,\n",
    "    return_tensors = 'pt',\n",
    ").input_ids.cuda()\n",
    "\n",
    "generation_output = model.generate(\n",
    "    tokens,\n",
    "    do_sample = False,\n",
    "    max_new_tokens = 512\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(generation_output[0],skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "cache_dir = '/content/drive/My Drive/huggingface_cache'\n",
    "os.makedirs(cache_dir, exist_ok= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install accelerate\n",
    "!pip install einops\n",
    "!pip install numpy\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer,AutoConfig,AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Py007/TinyLlama-1.1B-intermediate-step-480k-1T'\n",
    "\n",
    "model = AutoAWQCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code = True,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map = 'cpu',\n",
    "    offload_folder = 'offload',\n",
    "    cache_dir =  cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "def download_file_from_huggingface(model_name, filename, save_path):\n",
    "    url = f\"https://huggingface.co/{model_name}/resolve/main/{filename}\"\n",
    "    r = requests.get(url)\n",
    "    if r.status_code != 200:\n",
    "        print(f\"Failed to download {filename}.HTTP Status Code:\"{r.status_code}\n",
    "              return False\n",
    "    with open(os.path.join(save_path,filename), 'wb') as f:\n",
    "         f.write(r.content) )\n",
    "    return True\n",
    "\n",
    "\n",
    "def main():\n",
    "    files_to_download = [   \n",
    "        \"tokenizer_config.jason\",\n",
    "        \"tokenizer.model\",\n",
    "        \"tokenizer.json\",\n",
    "        \"special_tokens_map.json\",\n",
    "        \"added_tokens.json\"\n",
    "    ]\n",
    "\n",
    "    for filename in files_to_download:\n",
    "        success = download_file_from_huggingface(model_name, filename, save_path)\n",
    "        if success:\n",
    "            print(f\"Successfully downloaded {filename}\")\n",
    "        else:\n",
    "            print(f\"Failed to download {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt update -y\n",
    "!apt install build-essential git cmake libopenblas-dev libeigen3-dev -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!make LLAMA_OPENBLAS =1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python convert.py models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = model_name.split('/')\n",
    "\n",
    "model_name_pure = parts[1]\n",
    "\n",
    "quant_type = \"Q4_K\"\n",
    "quantized_model = f'models/{model_name_pure}.{quant_type}.gguf'\n",
    "print(f'Preparing {quantized_model} with {quant_type} quantization.')\n",
    "\n",
    "import subprocess\n",
    "\n",
    "command = [\"./quantize\", \"models/ggml-model-f16.gguf\", quantized_model, quant_type]\n",
    "\n",
    "subprocess.run(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after above now push the model to huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
